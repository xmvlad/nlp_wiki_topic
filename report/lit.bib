@inproceedings{Devlin2018,
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1810.04805v1},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
doi = {10.18653/v1/N19-1423},
eprint = {1810.04805v1},
file = {:Users/shanest/Documents/Library/Devlin et al/Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologie./Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
keywords = {model},
pages = {4171--4186},
publisher = {Association for Computational Linguistics},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://aclweb.org/anthology/N19-1423},
year = {2019}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{thain2017wikipedia,
  title={Wikipedia Talk Labels: Toxicity},
  author={Thain, Nithum, Dixon, Lucas, Wulczyn, Ellery},
  year={2017}
}

@article{rajpurkar2016squad,
  added-at = {2018-02-07T14:37:11.000+0100},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  biburl = {https://www.bibsonomy.org/bibtex/29baa1e3fcd561ea68f31e198e376dc71/thoni},
  interhash = {38da1f34ed178d9e375a1ca35cf87ecd},
  intrahash = {9baa1e3fcd561ea68f31e198e376dc71},
  journal = {arXiv preprint arXiv:1606.05250},
  keywords = {squad question answering solvatio dataset citedby:scholar:count:222 citedby:scholar:timestamp:2018-2-7},
  timestamp = {2018-02-07T14:37:11.000+0100},
  title = {Squad: 100,000+ questions for machine comprehension of text},
  year = 2016
}



@article{yinhan2019roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692}
}
